{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6894df3",
   "metadata": {},
   "source": [
    "![alt text](img/LM.png)\n",
    "# Kurs: Deep Learning, Text Mining i XAI w Pythonie\n",
    "\n",
    "## Autor: Piotr Ćwiakowski\n",
    "\n",
    "### Lekcja 10. Sieci neuronowe w NLP \n",
    "\n",
    "### Spis treści\n",
    "\n",
    "1. Word2Vec\n",
    "2. Przykład aplikacyjny "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea41ba",
   "metadata": {},
   "source": [
    "# 1 Word2vec\n",
    "\n",
    "## Wprowadzenie\n",
    "Reprezentacja słów (word embedding) jest wykonywana za pomocą modelu word2vec (skip-gram model). Polega na wytrenowaniu jednej prostej sieci neuronowej. Elementem sieci, który nas interesuje nie są jednak wartości dopasowane modelu, tylko wagi uzyskane w ukrytej warstwie. To one właśnie są wektorową reprezentacją słów, które potrzebujemy. Potrzebujemy zatem „fałszywego” zadania, żeby móc otrzymać wagi dla ukrytej warstwy. Tym zadaniem będzie przewidywanie prawdopodobieństwa wystąpienia w pobliżu danego słowa każdego innego słowa ze słownika. „Pobliże” jest często definiowane jako 5 słów przed i 5 słów po. Poniżej prosty przykład z 1 zdaniem i oknem -2/+2.\n",
    "\n",
    "<img src=\"img/W1.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d31ec8",
   "metadata": {},
   "source": [
    "Jak konkretnie trenowana jest sieć? Na podstawie wyprodukowanych z korpusu skipgramów tworzone są zestawy obserwacji (input, output), które następnie sekwencyjnie zasilają jedno warstwową MLP, która modeluje prawdopodobieństwo wystąpienia w kontekście słowa każdego z pozostałych wyrazów. Na przykładzie poniżej dla korpusu 10000 słów szacujemy sieć złożoną z 300 neuronów. Warstwa ukryta nie ma funkcji aktywacyjnej, ale końcowa warstwa używa funkcji softmax.\n",
    "\n",
    "<img src=\"img/W2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50daf4fc",
   "metadata": {},
   "source": [
    "Skupmy się teraz na warstwie ukrytej z przykładu z poprzedniego slajdu. W matematycznym sensie, redukuje ona liczbę wymiarów do 300. Wartościami w tych wymiarach są wartości uzyskane dla każdego słowa w warstwie ukrytej. Przyjrzyjmy się ilustracji:\n",
    "\n",
    "<img src=\"img/W3.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d7e36",
   "metadata": {},
   "source": [
    "## 2. Przykład aplikacyjny - IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a6dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie pakietów\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295caccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie danych\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e1035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               4000250   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Stworzenie modelu\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a4c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 6s - loss: 0.4430 - accuracy: 0.7719 - val_loss: 0.2926 - val_accuracy: 0.8765 - 6s/epoch - 31ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 6s - loss: 0.1689 - accuracy: 0.9365 - val_loss: 0.3166 - val_accuracy: 0.8732 - 6s/epoch - 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d7dc13dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estymacja\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f7559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.32%\n"
     ]
    }
   ],
   "source": [
    "# Ewaluacja\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20192415",
   "metadata": {},
   "source": [
    "# 3. Przykład aplikacyjny - word2vec w sieciach RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e06a43",
   "metadata": {},
   "source": [
    "### Pakiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e54f5",
   "metadata": {},
   "source": [
    "### Wczytanie i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be672083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = text_dataset_from_directory(\"./data/movie-reviews/train\")\n",
    "test_data = text_dataset_from_directory(\"./data/movie-reviews/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fdb5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepareData(dir):\n",
    "    data = text_dataset_from_directory(dir)\n",
    "    return data.map(lambda text, label: (regex_replace(text, '<br />', ' '), label))\n",
    "\n",
    "train_data = prepareData('./data/movie-reviews/train')\n",
    "test_data = prepareData('./data/movie-reviews/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02086ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Let\\'s begin with that theme song sung by Christopher Cross. The song is \"If you get caught between the moon and New York City.\" It\\'s a great theme and song even after all these years, it never gets tiring. It really is a great song about New York City as well. Anyway, the great Dudley Moore CBE stars as a spoiled drunken millionaire who is engaged to Jill Eikenberry\\'s character in the film. Jill would later star on LA Law. Anyway, he is served by his wonderful British butler, Sir John Gielgud OM who won an Academy Award for his performance in the film as Best Supporting Actor. Arthur falls in love with Liza Minnelli\\'s character who is perfect in this film besides her performance in her Oscar winning role in Cabaret. No, Liza doesn\\'t get to sing. She plays a diner waitress. Anyway I love Geraldine Fitzgerald as the Bach matriarch of the family who decides the family\\'s fortune. Anyway, she is fabulous and should have gotten an academy award nomination herself for Best Supporting Actress. Barney Martin best known as Jerry\\'s dad on Seinfeld plays Liza\\'s dad. He\\'s great too. The movie was well-written, acted, and delivered to the audience who wanted more of it.'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(1):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcac0c",
   "metadata": {},
   "source": [
    "### Architektura modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99dac9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f9e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 100, 128)          128128    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,761\n",
      "Trainable params: 181,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Maksymalny rozmiar słownika - w ten sposób ograniczamy liczbę zmiennych (do najbardziej popularnych), a wszystkie pozostałe słowa są zaklasyfikowane do kolumny out-of-vocabulary (OOV) \n",
    "  max_tokens=max_tokens,\n",
    "  # Wynikiem (outputem) będą liczby zamiast słów\n",
    "  output_mode=\"int\",\n",
    "  # Maksymalna liczba słów w każdej obserwacji - jeśli w tekscie jest wiecej - obcinamy, jesli mniej - wydłużamy (o zera)\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "# Jeśli tworzymy obiekt ze standaryzowanym słownikiem (max_tokens) - musimy go wytrenować na zbiorze\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "# Dodanie warstwy wektoryzującej tekst, nota bene słów jest 1000 + 1 (ostatnie to OOV)\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# Wykańczamy sieć neuronami rekurencyjnymi i feed-forward\n",
    "model.add(LSTM(64)) # warstwa rekurencyjna\n",
    "model.add(Dense(64, activation=\"relu\")) # warstwa feed-forward\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # warstwa wyjściowa\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a545d2e",
   "metadata": {},
   "source": [
    "### Kompilowanie modelu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e222a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e82c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 36s 44ms/step - loss: 0.5369 - accuracy: 0.7302\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.4446 - accuracy: 0.7952\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.4134 - accuracy: 0.8138\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 48s 61ms/step - loss: 0.3886 - accuracy: 0.8274\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 48s 62ms/step - loss: 0.3698 - accuracy: 0.8366\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 46s 59ms/step - loss: 0.3576 - accuracy: 0.8452\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3375 - accuracy: 0.8558\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3219 - accuracy: 0.8638\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.3052 - accuracy: 0.8704\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.2933 - accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d10a65630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model trenuje się kilka minut\n",
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee601ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 720ms/step\n",
      "[[0.98735964]]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "[[0.02483406]]\n"
     ]
    }
   ],
   "source": [
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
